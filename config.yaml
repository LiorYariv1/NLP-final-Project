
data_paths:
  cmu_plot_summaries: /home/student/project/data/plot_summaries.txt
  cmu_metadata: /home/student/project/data/movie.metadata.tsv
  wiki: /home/student/project/data/wiki_movie_plots_deduped.csv
  full_dataset: /home/student/project/data/full_dataset.csv
  filtered_dataset: /home/student/project/data/filtered_dataset.csv
  filtered_dataset_3_30: /home/student/project/data/filtered_dataset_3_30.csv
  filtered_dataset_3_30_new: /home/student/project/data/filtered_dataset_3_30_new.csv
  test_results: /home/student/project/data/test_results.csv

dataset:
  train_prcnt: 0.94 #percent of data for training
  test_prcnt: 0.02 #percent of data for test set
  # the remaining precentage will be validation set

T5:
  model_name: t5-base #used to load pre-trained tokenizer
  pretrained_model: t5-base #use this when training and model from scratch (i.e, for reproduction)
#  pretrained_model: '/home/student/project/model0303__kw_Rake_p3/' # (If you wish to load our final model)
  run_ds: filtered_dataset_3_30_new #the dataset input for the model, after all preprocess and train_test split
  input_cols: ['Title', 'new_genres'] #this is constant in our case.
#  model_save_path: /home/student/project/model0303__kw_Rake_p3 #where to save the model
  min_len: 3 #
  max_len: 30
  from_checkpoint: False # if True, load a trained model
                          #(from checkpoint/from saving final model)
                          # we used it for evaluation only, and the code implemented accordingly

  train_args:
#    output_dir: results
    overwrite_output_dir: True
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    save_strategy: epoch
    warmup_steps: 1000
    learning_rate: 1.0e-4
    evaluation_strategy: steps
    eval_steps: 1000
    do_train: True
    do_eval: True
    num_train_epochs: 10
    report_to: wandb

  train_args_checkpoint:
#    output_dir: results
    overwrite_output_dir: True
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    evaluation_strategy: steps
    eval_steps: 1000
    do_train: False
    do_eval: True
    num_train_epochs: 0
    report_to: wandb

w_and_b:
  project: NLP_project_PlotGenerator_finalrun
  entity: nlp_final_project
  group: run2
  job_type: rake
  mode: online